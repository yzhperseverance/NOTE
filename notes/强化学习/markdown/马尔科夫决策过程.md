# 马尔可夫决策过程

强化学习的目标是给定一个马尔可夫决策过程，寻找最优策略

## 马尔可夫过程

马尔可夫过程是一个二元组<S,P>。S是有限状态集，P是状态转移矩阵。马尔可夫过程是一个无记忆的随机过程，每个状态序列S1、S2具有**马尔可夫性质**，即未来只与现在有关而与过去无关。

## 状态转移矩阵

![image-20241116213234164](C:\Users\28609\AppData\Roaming\Typora\typora-user-images\image-20241116213234164.png)

 ## 马尔可夫奖励过程

由四元组<S,P,R,γ>组成

![image-20241116213649045](C:\Users\28609\AppData\Roaming\Typora\typora-user-images\image-20241116213649045.png)

​	设当前状态为$S_t=s$,由s到$S_{t+1}$集合的奖励值集合为$R_{t+1}$，$R_S$就是集合$R_{t+1}$的期望。$R_S$是一个期望，但$R_{t+1}$不是，他只是代表到达t+1时刻的某一个状态点的奖励。

![image-20241116214337520](C:\Users\28609\AppData\Roaming\Typora\typora-user-images\image-20241116214337520.png)

### 折扣因子γ

​	折扣因子γ是用来计算当前状态的总回报的，表示离的越远的时刻的价值越少。**注意，这里的$G_t$只是某一条路线的折扣奖励，并不是当前状态的最终价值。**

![image-20241116214657975](C:\Users\28609\AppData\Roaming\Typora\typora-user-images\image-20241116214657975.png)

### 值函数V(S)

V(S)表示状态S的长期价值，即综合了未来所有路线的折扣奖励，算出期望。

![image-20241116215121747](C:\Users\28609\AppData\Roaming\Typora\typora-user-images\image-20241116215121747.png)

### 贝尔曼方程

对于$G_t$可以采用递归的方式表示，即$G_t=R_{t+1} + γG_{t+1}$。因为原式是在s状态的，而要求$E[G(S_{t+1})]$要在s‘状态下，所以多了个转移矩阵。
$$
V(S)=E[R_{t+1}+\gamma G(s_{t+1})|S_t=s] \\
V(S)=R_s+\gamma \sum_{s'\in S}{P_{ss'}V(s')}
$$

#### 矩阵形式

==这里需要解释一下为什么两边看成一个V？==

- 首先所有的状态本质上都是对自己进行迭代更新，因为状态集是不变的。在标量形式下分开写是为了表达递归关系。

- 贝尔曼方程想表达一个平衡状态下的值函数，即当V达到平衡时（收敛），左右两侧一样

![image-20241117115225714](C:\Users\28609\AppData\Roaming\Typora\typora-user-images\image-20241117115225714.png)



## 马尔可夫决策过程

由一个五元组<S,==A==,P,R,γ表示>，其中A是有限动作集。

![image-20241116215625629](C:\Users\28609\AppData\Roaming\Typora\typora-user-images\image-20241116215625629.png)

### 策略

π是一个随机变量，他的分布代表所有动作。

![image-20241116215750732](C:\Users\28609\AppData\Roaming\Typora\typora-user-images\image-20241116215750732.png)

==当前S下采用不同a的可能性 * 采用每个a后到不同S'的可能性 = 在策略π下从S到S’的可能性==

![image-20241116220256616](C:\Users\28609\AppData\Roaming\Typora\typora-user-images\image-20241116220256616.png)

### 状态值函数

相比马尔可夫奖励过程的值函数，马尔可夫决策过程是智能体自身做出了对应动作后从而引起状态的改变，而前两者则是随波逐流。

![image-20241117111400125](C:\Users\28609\AppData\Roaming\Typora\typora-user-images\image-20241117111400125.png)

### 动作值函数

在状态S下，先采取策略a后回报的期望。注意，在S下采取状态a并不能到达某个确定的状态，还是具有状态转移矩阵$P_{S,S'}^{a}$。其实就是在执行该动作后的状态值函数。

![image-20241117112334695](C:\Users\28609\AppData\Roaming\Typora\typora-user-images\image-20241117112334695.png)

### 贝尔曼方程

$$
V(s)=\sum_{a \in A} \pi(a|s)[R_s^a + \gamma \sum_{s' \in S}P_{ss'}^aV(s')]\\
= \sum_{a \in A}\pi(a|s) q(s,a) \\
q(s,a)=R_s^a+\gamma \sum_{s' \in S}P_{ss'}^a \sum_{a' \in A}\pi(a'|s)q(s',a')\\
=R_s^a+\gamma \sum_{s' \in S}P_{ss'}^a v(s')
$$

### 状态值函数和动作值函数的关系

根据定义式可以推出：

![image-20241117122642600](C:\Users\28609\AppData\Roaming\Typora\typora-user-images\image-20241117122642600.png)

根据贝尔曼的递归式子可以推出：

![image-20241117122728273](C:\Users\28609\AppData\Roaming\Typora\typora-user-images\image-20241117122728273.png)

### 最优策略

![image-20241117123519960](C:\Users\28609\AppData\Roaming\Typora\typora-user-images\image-20241117123519960.png)